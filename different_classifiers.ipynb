{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# ========================================================\n",
    "# Custom file with methods for NN, cross-validation, etc\n",
    "# ========================================================\n",
    "from nn_methods import run_model, get_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used several different classifiers (excluding NN) in order to compare logloss for different approaches. Metrics value logloss is calculated with 5-fold stratified cross-validation procedure. \n",
    "\n",
    "Recall about the task: multi-class classification probelm, 39 classes, 900K rows in a train set, 900K in test set. Main features are spatial data (X,Y coordinates and different transformations) and date features (year, month, day,..). \n",
    "\n",
    "Data preparation and exploration is considered in sfcrime_data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = h5py.File(\"SFData.hdf5\", \"r\")\n",
    "Xtrain_d = f[\"X_train\"][:]\n",
    "ytrain = f[\"y_train\"][:]\n",
    "Xtest_d = f[\"X_test\"][:]\n",
    "dtest = f[\"X_test_ID\"][:]\n",
    "le_cat_class = f[\"le_cat_classes\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write results for submission\n",
    "def write_result(y_t, filename):\n",
    "    result = pd.DataFrame(y_t, index=dtest.Id)\n",
    "    result.columns = le_cat.classes_\n",
    "    result.to_csv(filename + '.csv', float_format='%.5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicies for stratified k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[702439, 702439, 702439, 702439, 702440]\n",
      "[175610, 175610, 175610, 175610, 175609]\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "indtrain1, indtrain2 = get_index(Xtrain_d, k)\n",
    "\n",
    "print map(lambda x: len(x), indtrain1)\n",
    "print map(lambda x: len(x), indtrain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878049, 57)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain_d.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General method for classifiers with using k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classifier_result(model, Xtrain_d, ytrain, indtrain1, indtrain2):\n",
    "    losses = []\n",
    "    allypred = np.zeros((len(Xtrain_d), 39)) # we will predict on each fold to make an ensemble\n",
    "    for ind in range(k):\n",
    "        print \"===== FOLD %s/4 ======\" % ind\n",
    "        ind_test = indtrain1[ind]  # ind1\n",
    "        ind_train = indtrain2[ind]  # ind2\n",
    "        \n",
    "        x_tr = Xtrain_d[ind_test,:]\n",
    "        y_tr = ytrain[ind_test]\n",
    "        \n",
    "        x_te = Xtrain_d[ind_test,:]\n",
    "        y_te = ytrain[ind_test]\n",
    "        \n",
    "        print \"Fitting...\"\n",
    "        model.fit(x_tr, y_tr)\n",
    "        \n",
    "        print \"Evaluating...\"\n",
    "        prob_te = model.predict_proba(x_te)\n",
    "        ll = log_loss(y_te, prob_te)\n",
    "        losses.append(ll)\n",
    "        print ll\n",
    "        allypred[ind_test, :] = prob_te\n",
    "    return allypred, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FOLD 0/4 ======\n",
      "Fitting...\n",
      "Evaluating...\n",
      "2.67145687663\n",
      "===== FOLD 1/4 ======\n",
      "Fitting...\n",
      "Evaluating...\n",
      "2.67035541355\n",
      "===== FOLD 2/4 ======\n",
      "Fitting...\n",
      "Evaluating...\n",
      "2.67112873925\n",
      "===== FOLD 3/4 ======\n",
      "Fitting...\n",
      "Evaluating...\n",
      "2.66960007376\n",
      "===== FOLD 4/4 ======\n",
      "Fitting...\n",
      "Evaluating...\n",
      "2.66967472531\n"
     ]
    }
   ],
   "source": [
    "# Don't recalculate! \n",
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "allypred, avg_loss = classifier_result(logreg, Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "\n",
    "# avg_logloss = 2.67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save to file...\n"
     ]
    }
   ],
   "source": [
    "print(\"Save to file...\")\n",
    "g = h5py.File(\"data_%s.hdf5\" % \"logreg\", \"w\")\n",
    "g.create_dataset(\"allypred\", data=allypred)\n",
    "g.create_dataset(\"avg_loss\", data=avg_loss)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final Log Regression.Training on all train set, calculate probabilities for test set \n",
    "logreg_final = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "logreg_final.fit(Xtrain_d, ytrain)\n",
    "prob_test = logreg_final.predict_proba(Xtest_d)\n",
    "write_result(prob_test, 'submission_logreg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FOLD 0/4 ======\n",
      "Fitting...\n"
     ]
    }
   ],
   "source": [
    "rand_forest = RandomForestClassifier(n_estimators=1000, max_depth=15)\n",
    "allypred, avg_loss = classifier_result(rand_forest, Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "\n",
    "# avg_logloss = 2.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Save to file...\")\n",
    "g = h5py.File(\"data_%s.hdf5\" % \"rand_forest\", \"w\")\n",
    "g.create_dataset(\"allypred\", data=allypred)\n",
    "g.create_dataset(\"avg_loss\", data=avg_loss)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final Random Forest. Training on all train set, calculate probabilities for test set \n",
    "rf_final = RandomForestClassifier(n_estimators=1000, max_depth=15)\n",
    "rf_final.fit(Xtrain_d, ytrain)\n",
    "prob_test = rf_final.predict_proba(Xtest_d)\n",
    "write_result(prob_test, 'submission_rf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_xg_boost_result(Xtrain_d, ytrain, indtrain1, indtrain2):\n",
    "    param = {}\n",
    "    param['booster'] = 'gbtree'\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['num_class'] = 39\n",
    "    param['eval_metric'] = 'logloss'\n",
    "    # param['scale_pos_weight'] = 1.0\n",
    "    param['bst:eta'] = 1\n",
    "    param['bst:max_depth'] = 6\n",
    "    # param['bst:colsample_bytree'] = 0.4\n",
    "    # param['gamma'] = 0.5\n",
    "    # param['min_child_weight'] = 5.\n",
    "    param['max_delta_step'] = 1\n",
    "    # param['silent'] = 1\n",
    "    # param['nthread'] = 30\n",
    "    num_round = 15\n",
    "    plst = list(param.items())\n",
    "    watchlist = []\n",
    "\n",
    "    losses = []\n",
    "    allypred = np.zeros((len(Xtrain_d), 39)) # we will predict on each fold to make an ensemble\n",
    "    for ind in range(k):\n",
    "        print \"===== FOLD %s/4 ======\" % ind\n",
    "        ind_test = indtrain1[ind]  # ind1\n",
    "        ind_train = indtrain2[ind]  # ind2\n",
    "        \n",
    "        x_tr = Xtrain_d[ind_test,:]\n",
    "        y_tr = ytrain[ind_test]\n",
    "        \n",
    "        x_te = Xtrain_d[ind_test,:]\n",
    "        y_te = ytrain[ind_test]\n",
    "        \n",
    "        dtrain_x = xgb.DMatrix(x_tr, label=y_tr)\n",
    "        dtest_x = xgb.DMatrix(x_te, label=y_te)\n",
    "\n",
    "        print \"Fitting...\"\n",
    "        bst = xgb.train(plst, dtrain_x, num_round, watchlist)\n",
    "        bst.save_model(\"xgboost_%s.model\" % str(ind))\n",
    "        \n",
    "        print \"Evaluating...\"\n",
    "        curpred = bst.predict(dtest_x)\n",
    "        ll = log_loss(y_te, curpred)\n",
    "        \n",
    "        print ll\n",
    "        losses.append(ll)\n",
    "        allypred[ind_test, :] = curpred\n",
    "    \n",
    "    return allypred, np.mean(losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# average logloss for XgBoost\n",
    "allypred, avg_loss = classifier_xg_boost_result(Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "\n",
    "# avg. logloss for 70% of train set = 2.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Save to file...\")\n",
    "g = h5py.File(\"data_%s.hdf5\" % \"xgboost\", \"w\")\n",
    "g.create_dataset(\"allypred\", data=allypred)\n",
    "g.create_dataset(\"avg_loss\", data=avg_loss)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Final model for XgBoost, train on all data\n",
    "\n",
    "dtrain_x = xgb.DMatrix(Xtrain_d, label=ytrain)\n",
    "print \"Fitting...\"\n",
    "bst = xgb.train(plst, dtrain_x, num_round, watchlist)\n",
    "bst.save_model(\"final_xgboost.model\")\n",
    "curpred = bst.predict(Xtest_d)\n",
    "write_result(curpred, 'submission_xgboost')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FOLD 0/4 ======\n",
      "Fitting...\n",
      "Evaluating...\n"
     ]
    }
   ],
   "source": [
    "knn_classifier4 = KNeighborsClassifier(n_neighbors=4)\n",
    "knn_classifier8 = KNeighborsClassifier(n_neighbors=8)\n",
    "knn_classifier16 = KNeighborsClassifier(n_neighbors=16)\n",
    "knn_classifier32 = KNeighborsClassifier(n_neighbors=32)\n",
    "knn_classifier64 = KNeighborsClassifier(n_neighbors=64)\n",
    "\n",
    "\n",
    "allypred4, avg_loss4 = classifier_result(knn_classifier4, Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "allypred8, avg_loss8 = classifier_result(knn_classifier8, Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "allypred16, avg_loss16 = classifier_result(knn_classifier16, Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "allypred32, avg_loss32 = classifier_result(knn_classifier32, Xtrain_d, ytrain, indtrain1, indtrain2)\n",
    "allypred64, avg_loss64 = classifier_result(knn_classifier64, Xtrain_d, ytrain, indtrain1, indtrain2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print avg_loss4, avg_loss8, avg_loss16, avg_loss32, avg_loss64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
